{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pysolation course project #1\n",
    "27th of May 2020\n",
    "Tonya White, Vera Bouwman, Louk de Mol, Elisabet Blok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first we will import all the modules that we are going to need for this script. <br> **nibabel** is the package that we will use to import the files that we obtain from FreeSurfer. <br> **numpy** is a package that allows us to do vector and matrix operations. <br> **pandas** is a package that lets us work with large amounts of data in so-called dataframes. <br> **matplotlib.pyplot** is a package that lets us plot data. <br> **scipy.stats** is a package that lets us do statistics on our data. <br> **nibabel.freesurfer.mghformat.load** is a specific function that allows us to easily load FreeSurfer formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 'as' makes it easier for us to use a package throughout a file,\n",
    "# it will allow us to use functions in that package using nib.FUNCTION() instead of using nibabel.FUNCTION()\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Importing 'from' means that we only use a specific subpart of a package\n",
    "# this can be a function or a structure, like scipy.stats that contains specific functions\n",
    "# instead of loading all of the functions inside scipy, \n",
    "# we can load a subpackage that only contains statistics functions\n",
    "from scipy import stats\n",
    "from nibabel import freesurfer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORDER OF FUNCTIONALITY IN THIS SCRIPT\n",
    "1. Read in IDC paths\n",
    "2. Check how large output arrays should be\n",
    "3. Make output data arrays\n",
    "4. Array for LH thickness cases and array LH thickness controls (similar for all following outputs)\n",
    "5. Make loop to cycle through all vertices \n",
    "6. Run paired ttest\n",
    "7. Write output \n",
    "8. Go to next morph\n",
    "9. Apply multiple testing (and figure out how!) --> check with Sander\n",
    "\n",
    "### Loop order\n",
    "1. Length of morph\n",
    "2. Cycle through vertices\n",
    "\n",
    "Why this way? To circumvent having to load the data every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data on IDC with in the first column the ASD cases and in the second column the matched controls\n",
    "# we use the following file to read the IDCs of the patients that we are going to use\n",
    "idc_file_path = Path('IDC_case_control.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first path refers to the folder that the subjects imaging data, thickness data, etc. are in\n",
    "data_path = Path('data/')\n",
    "\n",
    "# The second path refers to a specific sub-path that we want to use, which contains the surface data\n",
    "# from FreeSurfer\n",
    "surface_path = Path('surf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list of the MGH files for the thickness, area and local gyrification index of the cortex.\n",
    "morph = [Path('lh.thickness'), # The left hemisphere thickness per vertex\n",
    "         Path('rh.thickness'), # The right hemisphere thicknes per vertex\n",
    "         Path('lh.area'), # The left hemisphere area\n",
    "         Path('rh.area')] # The right hemisphere area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also select an output path that we use to store the results of our tests.\n",
    "output_path = Path('output_path')\n",
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the matched input IDCs\n",
    "idc_matrix = np.genfromtxt(data_path / idc_file_path,    # The input file path\n",
    "                           delimiter=',', # The delimiter that is used to split each element in the file \n",
    "                           dtype=np.str)  # The data type, in this case they are strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '5' '8' '10']\n"
     ]
    }
   ],
   "source": [
    "print(idc_matrix[:, 0]) # These are the IDCs for the ASD cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4' '7' '12' '3']\n"
     ]
    }
   ],
   "source": [
    "print(idc_matrix[:, 1]) # These are the IDCs for the control cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/1/surf/lh.thickness\n"
     ]
    }
   ],
   "source": [
    "# The path of morphological structures is the absolute data path, \n",
    "# the surface path and then the specific morphological path\n",
    "lh_thickness_idc1_path = data_path / Path(idc_matrix[0, 0]) / surface_path / morph[0]\n",
    "\n",
    "# We use the absolute data path that all the cases are in, each case is in a directory with their IDC,\n",
    "# in this case the IDC is '1', so we get absolute_data_path/1, then we go into the surf/ directory,\n",
    "# so we get absolute_data_path/1/surf/ and then we select a morphological file we want to open for this case\n",
    "# which in this case is the left hemisphere thickness,\n",
    "# so we get absolute_data_path/1/surf/lh.thickness.fwhm10.fsaverage.mgh\n",
    "print(lh_thickness_idc1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163842,)\n"
     ]
    }
   ],
   "source": [
    "# We can now load the thickness data for this case using a function that we imported\n",
    "lh_thickness_idc1_data = freesurfer.read_morph_data(lh_thickness_idc1_path)\n",
    "\n",
    "# We can look at the shape of the data to see how many vertices there are\n",
    "print(lh_thickness_idc1_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of IDCs: 4, \n",
      "number of morphological data structures: 4,\n",
      "and number of vertices: 163842\n"
     ]
    }
   ],
   "source": [
    "num_idcs = idc_matrix.shape[0] # The number of IDCs is the same as the rows in the idc_matrix\n",
    "num_morphs = len(morph) # The number of morphological data structures we want to analyze\n",
    "num_vertices = lh_thickness_idc1_data.shape[0] # The number of vertices there are\n",
    "print(f'Number of IDCs: {num_idcs}, \\n'\n",
    "      f'number of morphological data structures: {num_morphs},\\n' \n",
    "      f'and number of vertices: {num_vertices}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is going to be the main loop, where we loop over each morphological structure\n",
    "\n",
    "We loop over each morphological file in our morph list and then write the results of our statistical tests to MGH files in our output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current morphological file we are analyzing is: lh.thickness\n",
      "[ 1.88316912  1.19744537 -0.25838805 ...  0.46091048  1.0257171\n",
      " -0.13127626]\n",
      "The current morphological file we are analyzing is: rh.thickness\n",
      "[-0.4755626  -0.5134345  -0.00547918 ... -0.09016326 -0.2183428\n",
      " -0.97300827]\n",
      "The current morphological file we are analyzing is: lh.area\n",
      "[-1.12923819 -0.64973593  0.32310168 ... -0.05051875  2.4087538\n",
      " -0.29306811]\n",
      "The current morphological file we are analyzing is: rh.area\n",
      "[ 0.12789462 -0.04008177  0.46179237 ...  1.45463431  0.68636211\n",
      "  0.22948028]\n"
     ]
    }
   ],
   "source": [
    "for morph_file in morph: # Loop over each morphological file that we have in our list\n",
    "    \n",
    "    print(f'The current morphological file we are analyzing is: {morph_file}')\n",
    "    \n",
    "    data_t = np.zeros( (num_vertices) ) # We create a three-dimensional array for t-values for each vertex\n",
    "    data_p = np.zeros( (num_vertices) ) # We create a three-dimensional array for p-values for each vertex\n",
    "    \n",
    "    data_cases = np.zeros( (num_idcs, num_vertices) ) # We create a three-dimensional array for each case\n",
    "    data_controls = np.zeros( (num_idcs, num_vertices) ) # We create a three-dimensional array for each control\n",
    "    \n",
    "    for idc_index in range(num_idcs): # Loop over each IDC pair, using an index for the rows in idc_matrix\n",
    "        \n",
    "        # Create the data path and then load the data for the case\n",
    "        case_data_path = data_path / Path(idc_matrix[idc_index, 0]) / surface_path / morph_file\n",
    "        case_data = freesurfer.read_morph_data(case_data_path)\n",
    "        \n",
    "        # Create the data path and then load the data for the control\n",
    "        control_data_path = data_path / Path(idc_matrix[idc_index, 1]) / surface_path / morph_file\n",
    "        control_data = freesurfer.read_morph_data(control_data_path)\n",
    "        \n",
    "        # Add the vertices data, based on the morphological file that we are accessing,\n",
    "        # to the arrays that we just created for the cases and the controls\n",
    "        data_cases[idc_index, :] = case_data\n",
    "        data_controls[idc_index, :] = control_data\n",
    "    \n",
    "    # Look for the maximum values for the vertices, which in this case is scanning for the maximum value \n",
    "    # across the IDCs, which is why we use axis=0\n",
    "    max_vert_data_cases = np.max(data_cases,\n",
    "                                 axis=0)\n",
    "    max_vert_data_controls = np.max(data_controls,\n",
    "                                    axis=0)\n",
    "    \n",
    "    # Then we get the indices at which the maximum vertices for both cases and controls is above 0\n",
    "    selected_vertices = np.where( (max_vert_data_cases > 0) & (max_vert_data_controls > 0))[0]\n",
    "    \n",
    "    # We use the indices of the vertices that are not 0 and only use those vertices to do analyses\n",
    "    data_cases_selected = data_cases[:, selected_vertices]\n",
    "    data_controls_selected = data_controls[:, selected_vertices]\n",
    "    \n",
    "    # Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
    "    # (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html)\n",
    "    # The output to this function are two arrays of size (selected_vertices, )\n",
    "    # The first output are the t-values or statistics and the second output are the p_values for each vertex\n",
    "    statistic, p_value = stats.ttest_rel(data_cases_selected,\n",
    "                                         data_controls_selected,\n",
    "                                         axis=0)\n",
    "    \n",
    "    # Assign the statistic, also known as the t-statistic to our data_t array\n",
    "    # Assign the p values to the data_p array\n",
    "    data_t[selected_vertices] = statistic\n",
    "    data_p[selected_vertices] = 1 - p_value\n",
    "    \n",
    "    print(data_t)\n",
    "    \n",
    "    # Create an MGHImage using the t-statistics and the p-values\n",
    "    output_ttest_t = nib.MGHImage(data_t.astype(np.float32), \n",
    "                                  affine = None)\n",
    "    output_ttest_p = nib.MGHImage(data_p.astype(np.float32), \n",
    "                                  affine = None)\n",
    "    \n",
    "    # Save the MGHImages to the output path with an extension, based on what type of data it is.\n",
    "    nib.save(output_ttest_t, \n",
    "             f'{output_path / morph_file}out_t.mgh')\n",
    "    nib.save(output_ttest_p, \n",
    "             f'{output_path / morph_file}out_p.mgh')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
